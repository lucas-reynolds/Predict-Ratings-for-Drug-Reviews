{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "#### Project Overview\n",
    "Build a model that can predict whether or not a rating will be \"good\" (8 or higher) based on the text of a drug review.\n",
    "\n",
    "#### Goal\n",
    "I have already gone through steps for data wrangling, storytelling, and statistical analysis. My goal for this final step is to evaluate the performance of several models and choose the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "# data import/export\n",
    "from scipy.sparse import load_npz\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe\n",
    "data = pd.read_pickle('drugsCom_data')\n",
    "\n",
    "# import term matrix\n",
    "term_matrix = load_npz('ngram_csr.npz')\n",
    "\n",
    "# convert term matrix to dataframe\n",
    "term_matrix = pd.DataFrame(term_matrix.todense())\n",
    "\n",
    "# import column headers\n",
    "pickle_in = open('list.pickle', 'rb')\n",
    "reviews_columns = pickle.load(pickle_in)\n",
    "\n",
    "# add column headers to term matrix\n",
    "term_matrix.columns = reviews_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "To keep the model simple, I will convert the review ratings from a scale of 1 to 10 to binary. 8 and up will be labeled 1 and below 8 will be labeled 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rating scale to binary; >= 8 is 1, <8 is 0\n",
    "replace_values = {1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:1, 9:1, 10:1}\n",
    "term_matrix['rating'] = term_matrix.rating.replace(replace_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays for response variable and features\n",
    "y = term_matrix['rating'].values\n",
    "X = term_matrix.drop('rating', axis=1).values\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier\n",
    "Using a dummy classifier will provide a baseline for comparing subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:\n",
      " 0.5\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[    0 12751]\n",
      " [    0 19149]]\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     12751\n",
      "           1       0.60      1.00      0.75     19149\n",
      "\n",
      "    accuracy                           0.60     31900\n",
      "   macro avg       0.30      0.50      0.38     31900\n",
      "weighted avg       0.36      0.60      0.45     31900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasreynolds/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dummy_model = DummyClassifier(strategy='most_frequent', random_state=9)\n",
    "dummy_model.fit(X_train, y_train)\n",
    "y_pred = dummy_model.predict(X_test)\n",
    "print('AUC Score:\\n', roc_auc_score(y_test, y_pred))\n",
    "print('\\n\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "print('\\n\\nClassification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes (NB) maintains simplicity and speed by assuming all features (words in this case) are independent. The algorithm performs well in a lot of cases, particularly classification problems from text. I think NB will perform moderately well in this case, but a more robust model, such as Random Forest, will probably be best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:\n",
      " 0.6534088172302402\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 6376  6375]\n",
      " [ 3700 15449]]\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.50      0.56     12751\n",
      "           1       0.71      0.81      0.75     19149\n",
      "\n",
      "    accuracy                           0.68     31900\n",
      "   macro avg       0.67      0.65      0.66     31900\n",
      "weighted avg       0.68      0.68      0.68     31900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate model, fit, and predict\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "y_pred = naive_bayes.predict(X_test)\n",
    "\n",
    "# print AUC score\n",
    "print('AUC Score:\\n', roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# print confusion matrix\n",
    "print('\\n\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# print classification report\n",
    "print('\\n\\nClassification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score of 0.65 is pretty good for a first run. Again, I think NB is a great option when speed and simplicity are top priority, but I'd like to see a higher AUC score.\n",
    "\n",
    "Let's try Logistic Regression next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:\n",
      " 0.6683542915103206\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 6309  6442]\n",
      " [ 3027 16122]]\n",
      "\n",
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.49      0.57     12751\n",
      "           1       0.71      0.84      0.77     19149\n",
      "\n",
      "    accuracy                           0.70     31900\n",
      "   macro avg       0.70      0.67      0.67     31900\n",
      "weighted avg       0.70      0.70      0.69     31900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate model, fit, and predict\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# print AUC score\n",
    "print(f'AUC Score:\\n {roc_auc_score(y_test, y_pred)}')\n",
    "\n",
    "# print confusion matrix\n",
    "print('\\n\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# print classification report\n",
    "print('\\n\\n Classification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logisic Regression was a small improvement compared to Naive Bayes. \n",
    "\n",
    "I believe Random Forest will outperform both of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.8256836449100752\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 9369  3382]\n",
      " [ 1597 17552]]\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.79     12751\n",
      "           1       0.84      0.92      0.88     19149\n",
      "\n",
      "    accuracy                           0.84     31900\n",
      "   macro avg       0.85      0.83      0.83     31900\n",
      "weighted avg       0.84      0.84      0.84     31900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate model, fit, and predict\n",
    "r_forest = RandomForestClassifier(n_jobs=1, random_state=123)\n",
    "r_forest.fit(X_train, y_train)\n",
    "y_pred = r_forest.predict(X_test)\n",
    "\n",
    "# print AUC score\n",
    "print(f'AUC score: {roc_auc_score(y_test, y_pred)}')\n",
    "\n",
    "# print confusion matrix\n",
    "print(f'\\n\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# print classification report\n",
    "print('\\n\\nClassification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 80, 'n_estimators': 200}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set params to evaluate\n",
    "params = {'n_estimators': [150, 200, 250],\n",
    "          'max_depth': [40, 60, 80]}\n",
    "\n",
    "# instantiate and fit GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=r_forest,\n",
    "                           param_grid=params)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print best parameters\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.8256552301527968\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 9319  3432]\n",
      " [ 1523 17626]]\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.73      0.79     12751\n",
      "           1       0.84      0.92      0.88     19149\n",
      "\n",
      "    accuracy                           0.84     31900\n",
      "   macro avg       0.85      0.83      0.83     31900\n",
      "weighted avg       0.85      0.84      0.84     31900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate model with best params\n",
    "r_forest = RandomForestClassifier(max_depth=90,\n",
    "                                  n_estimators=140, \n",
    "                                  n_jobs=1, \n",
    "                                  random_state=123)\n",
    "# fit and predict\n",
    "r_forest.fit(X_train, y_train)\n",
    "y_pred = r_forest.predict(X_test)\n",
    "\n",
    "# print AUC score\n",
    "print(f'AUC score: {roc_auc_score(y_test, y_pred)}')\n",
    "\n",
    "# print confusion matrix\n",
    "print(f'\\n\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# print classification report\n",
    "print('\\n\\nClassification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
