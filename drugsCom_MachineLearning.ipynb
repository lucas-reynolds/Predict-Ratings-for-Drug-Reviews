{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "#### Project Overview\n",
    "Build a model that can predict whether or not a rating will be \"good\" (8 or higher) based on the text of a drug review.\n",
    "\n",
    "#### Goal\n",
    "I have already gone through steps for data wrangling, storytelling, and statistical analysis. My goal for this final step is to evaluate the performance of several models and choose the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "\n",
    "# data import/export\n",
    "from scipy.sparse import load_npz\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe\n",
    "data = pd.read_pickle('drugsCom_data')\n",
    "\n",
    "# import term matrix\n",
    "term_matrix = load_npz('ngram_csr.npz')\n",
    "\n",
    "# convert term matrix to dataframe\n",
    "term_matrix = pd.DataFrame(term_matrix.todense())\n",
    "\n",
    "# import column headers\n",
    "pickle_in = open('list.pickle', 'rb')\n",
    "reviews_columns = pickle.load(pickle_in)\n",
    "\n",
    "# add column headers to term matrix\n",
    "term_matrix.columns = reviews_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "To keep the model simple, I will convert the review ratings from a scale of 1 to 10 to binary. 8 and up will be labeled 1 and below 8 will be labeled 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rating scale to binary; >= 8 is 1, <8 is 0\n",
    "replace_values = {1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:1, 9:1, 10:1}\n",
    "term_matrix['rating'] = term_matrix.rating.replace(replace_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays for independent features and target\n",
    "X = term_matrix.drop('rating', axis=1).values\n",
    "y = term_matrix['rating'].values\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier\n",
    "Using a dummy classifier will provide a baseline for comparing subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score:\n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "# fit and evaluate dummy classifier\n",
    "dummy_model = DummyClassifier(strategy='most_frequent', random_state=123)\n",
    "dummy_model.fit(X_train, y_train)\n",
    "y_pred = dummy_model.predict(X_test)\n",
    "\n",
    "print('AUC Score:\\n', roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes (NB) maintains simplicity and speed by assuming all features (words in this case) are independent. The algorithm performs well in a lot of cases, particularly classification problems from text. I think NB will perform moderately well in this case, but a more robust model, such as Random Forest, will probably be best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for each fold: [0.73202462 0.73090894 0.72994204 0.73379731 0.73711134]\n",
      "Mean AUC: 0.7327568507938473\n"
     ]
    }
   ],
   "source": [
    "# instantiate model, fit, and predict\n",
    "naive_bayes = MultinomialNB()\n",
    "cv_results = cross_val_score(naive_bayes, \n",
    "                             X_train, y_train, \n",
    "                             scoring='roc_auc',\n",
    "                             cv=5)\n",
    "\n",
    "print(f'AUC for each fold: {cv_results}')\n",
    "print(f'Mean AUC: {np.mean(cv_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AUC score of 0.73 is pretty good for a first run with Naive Bayes. I'm also going to try Logistic Regression and Random Forest. I'm pretty sure Random Forest will outperform the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for each fold: [0.7555984  0.75598477 0.75277187 0.75744846 0.76285309]\n",
      "Mean AUC: 0.756931317872261\n"
     ]
    }
   ],
   "source": [
    "# instantiate model, fit, and predict\n",
    "log_reg = LogisticRegression()\n",
    "cv_results = cross_val_score(log_reg, \n",
    "                             X_train, y_train, \n",
    "                             scoring='roc_auc',\n",
    "                             cv=5)\n",
    "\n",
    "print(f'AUC for each fold: {cv_results}')\n",
    "print(f'Mean AUC: {np.mean(cv_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logisic Regression was a small improvement compared to Naive Bayes bringing our AUC from 0.73 up to 0.76."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for each fold: [0.89825974 0.89765881 0.89651376 0.9011878  0.90180346]\n",
      "Mean AUC: 0.8990847120332537\n"
     ]
    }
   ],
   "source": [
    "# instantiate model, fit, and predict\n",
    "r_forest = RandomForestClassifier(n_jobs=1, random_state=123)\n",
    "cv_results = cross_val_score(r_forest, \n",
    "                             X_train, y_train, \n",
    "                             scoring='roc_auc',\n",
    "                             cv=5)\n",
    "\n",
    "print(f'AUC for each fold: {cv_results}')\n",
    "print(f'Mean AUC: {np.mean(cv_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest outperformed Naive Bayes and Logistic Regression even with the default hyperparameters. I want to see if I can increase the performance using GridSearchCV to tune n_estimators and max_depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 80, 'n_estimators': 300}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set params to evaluate\n",
    "params = {'n_estimators': [150, 200, 250, 300],\n",
    "          'max_depth': [40, 60, 80, 100]}\n",
    "\n",
    "# instantiate and fit GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=r_forest,\n",
    "                           param_grid=params)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print best parameters\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for each fold: [0.92945266 0.92456645 0.92926356 0.92874972 0.92746518]\n",
      "Mean AUC: 0.9278995153465189\n"
     ]
    }
   ],
   "source": [
    "# re-run random forest with best parameters\n",
    "r_forest = RandomForestClassifier(n_estimators=300,\n",
    "                                  max_depth=80,\n",
    "                                  n_jobs=1, \n",
    "                                  random_state=123)\n",
    "cv_results = cross_val_score(r_forest, \n",
    "                             X, y, \n",
    "                             scoring='roc_auc',\n",
    "                             cv=5)\n",
    "\n",
    "print(f'AUC for each fold: {cv_results}')\n",
    "print(f'Mean AUC: {np.mean(cv_results)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
